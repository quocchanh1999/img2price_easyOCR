import streamlit as st
import pandas as pd
import numpy as np
import joblib
import re
from rapidfuzz import process, fuzz
import unicodedata
import nltk
from nltk.stem.snowball import SnowballStemmer
import unicodedata
import easyocr
from PIL import Image
import io
import asyncio
import sys
import io
import google.generativeai as genai
import os

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

if sys.platform.startswith("win"):
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

st.set_page_config(layout="centered", page_title="D·ª± ƒëo√°n Gi√° thu·ªëc")

@st.cache_resource

def initialize_tools():
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')

    stemmer = SnowballStemmer("english")
    reader = easyocr.Reader(['vi', 'en'], gpu=False) 

    return stemmer, reader

stemmer, ocr_reader = initialize_tools()

@st.cache_resource
def load_artifacts():
    try:
        model = joblib.load(os.path.join(BASE_DIR, "final_model.joblib"))
        scaler = joblib.load(os.path.join(BASE_DIR, "scaler.joblib"))
        target_maps = joblib.load(os.path.join(BASE_DIR, "target_encoding_maps.joblib"))
        mean_price = joblib.load(os.path.join(BASE_DIR, "global_mean_price.joblib"))
        df_full = pd.read_excel(os.path.join(BASE_DIR, "dichvucong_medicines_Final.xlsx"))
        train_cols = pd.read_csv(os.path.join(BASE_DIR, "X_train_price_processed.csv")).columns.tolist()
        return model, scaler, target_maps, mean_price, df_full, train_cols
    except FileNotFoundError as e:
        st.error(f"Loaded file error.")
        st.stop()


REPLACEMENTS_DNSX = { 'ctcp': 'c√¥ng ty c·ªï ph·∫ßn', 'tnhh': 'tr√°ch nhi·ªám h·ªØu h·∫°n', 'dp': 'd∆∞·ª£c ph·∫©m', 'tw': 'trung ∆∞∆°ng', 'cty': 'c√¥ng ty', 'ct': 'c√¥ng ty', 'cp': 'c·ªï ph·∫ßn', 'sx': 's·∫£n xu·∫•t', 'tm': 'th∆∞∆°ng m·∫°i', 'ld': 'li√™n doanh', 'mtv': 'm·ªôt th√†nh vi√™n' }
GENERIC_TERMS_DNSX = [ 'c√¥ng ty c·ªï ph·∫ßn', 'c√¥ng ty tnhh', 'c√¥ng ty', 'tr√°ch nhi·ªám h·ªØu h·∫°n', 'm·ªôt th√†nh vi√™n', 'li√™n doanh', 'c·ªï ph·∫ßn', 's·∫£n xu·∫•t', 'th∆∞∆°ng m·∫°i', 'trung ∆∞∆°ng', 'limited', 'ltd', 'pvt', 'inc', 'corp', 'corporation', 'gmbh', 'co', 'kg', 'ag', 'srl', 'international', 'pharma', 'pharmaceuticals', 'pharmaceutical', 'laboratories', 'industries' ]
GENERIC_TERMS_DNSX.sort(key=len, reverse=True)

def ultimate_company_name_cleaner(name_series, stemmer):
    def clean_single_name(name):
        name = str(name).lower()
        name = re.sub(r'\([^)]*\)', '', name)
        name = re.sub(r'[^a-z0-9\s]', ' ', name)
        name = re.sub(r'\s+', ' ', name).strip()
        for old, new in REPLACEMENTS_DNSX.items():
            name = name.replace(old, new)
        tokens = name.split()
        stemmed_tokens = [stemmer.stem(token) for token in tokens if token]
        name = " ".join(stemmed_tokens)
        for term in GENERIC_TERMS_DNSX:
            stemmed_term = " ".join([stemmer.stem(t) for t in term.split()])
            if stemmed_term:
                name = name.replace(stemmed_term, '')
        return re.sub(r'\s+', ' ', name).strip()
    return name_series.apply(clean_single_name)

DEFINITIVE_DBC_MAP = {
    'Thu·ªëc c·∫•y/Que c·∫•y': ['c·∫•y d∆∞·ªõi da', 'que c·∫•y'],
    'D·∫°ng x·ªãt d∆∞·ªõi l∆∞·ª°i': ['x·ªãt d∆∞·ªõi l∆∞·ª°i'],
    'Kh√≠ dung/H√≠t': ['kh√≠ dung', 'aerosol', 'inhaler', 'h√≠t', 'phun m√π'],
    'Thu·ªëc ƒë·∫∑t': ['thu·ªëc ƒë·∫°n', 'vi√™n ƒë·∫∑t', 'ƒë·∫°n ƒë·∫∑t', 'suppository', 'vi√™n ƒë·∫°n'],
    'Thu·ªëc g√¢y m√™ ƒë∆∞·ªùng h√¥ h·∫•p': ['g√¢y m√™', 'h√¥ h·∫•p'],
    'Tr√† t√∫i l·ªçc': ['tr√† t√∫i l·ªçc'],
    'B·ªôt pha ti√™m/truy·ªÅn': ['b·ªôt ƒë√¥ng kh√¥ pha ti√™m', 'b·ªôt pha ti√™m', 'powder for injection', 'b·ªôt v√† dung m√¥i pha ti√™m', 'b·ªôt ƒë√¥ng kh√¥', 'dung m√¥i pha ti√™m', 'b·ªôt v√¥ khu·∫©n pha ti√™m'],
    'Dung d·ªãch ti√™m/truy·ªÅn': ['dung d·ªãch ti√™m', 'thu·ªëc ti√™m', 'b∆°m ti√™m', 'injection', 'solution for injection', 'd·ªãch truy·ªÅn', 'd·ªãch treo v√¥ khu·∫©n', 'l·ªç', '·ªëng', 'dung dich ti√™m'],
    'H·ªón d·ªãch ti√™m/truy·ªÅn': ['h·ªón d·ªãch ti√™m', 'suspension for injection'],
    'Nh≈© t∆∞∆°ng ti√™m/truy·ªÅn': ['nh≈© t∆∞∆°ng ti√™m', 'emulsion for injection'],
    'Ho√†n (YHCT)': ['ho√†n m·ªÅm', 'ho√†n c·ª©ng', 'vi√™n ho√†n'],
    'Cao l·ªèng (YHCT)': ['cao l·ªèng'],
    'Cao xoa/d√°n (YHCT)': ['cao xoa', 'cao d√°n'],
    'D·∫ßu xoa/gi√≥': ['d·∫ßu xoa', 'd·∫ßu gi√≥', 'd·∫ßu xoa b√≥p', 'd·∫ßu b√¥i ngo√†i da'],
    'Kem b√¥i da': ['kem b√¥i', 'kem', 'cream'],
    'Gel b√¥i da': ['gel b√¥i', 'gel'],
    'Thu·ªëc m·ª° b√¥i da': ['thu·ªëc m·ª°', 'ointment', 'thu√¥ÃÅc m∆°ÃÉ', 'm·ª° b√¥i da', 'm·ª° b√¥i ngo√†i da'],
    'Mi·∫øng d√°n': ['mi·∫øng d√°n', 'patch'],
    'Lotion': ['lotion'],
    'C·ªìn/R∆∞·ª£u thu·ªëc': ['c·ªìn thu·ªëc', 'c·ªìn xoa b√≥p', 'r∆∞·ª£u thu·ªëc'],
    'N∆∞·ªõc s√∫c mi·ªáng/R∆° mi·ªáng': ['n∆∞·ªõc s√∫c mi·ªáng', 'r∆° mi·ªáng'],
    'D·∫ßu g·ªôi': ['d·∫ßu g·ªôi'],
    'Dung d·ªãch nh·ªè (M·∫Øt/M≈©i/Tai)': ['nh·ªè m·∫Øt', 'nh·ªè m≈©i', 'nh·ªè tai', 'eye drops', 'nasal drops', 'dung diÃ£ch nhoÃâ mƒÉÃÅt'],
    'Dung d·ªãch x·ªãt (M≈©i/Tai)': ['x·ªãt m≈©i', 'x·ªãt', 'nasal spray', 'spray'],
    'Thu·ªëc m·ª° (M·∫Øt/M≈©i/Tai)': ['m·ª° tra m·∫Øt', 'eye ointment'],
    'Vi√™n nang': ['vi√™n nang', 'nang', 'capsule', 'cap'],
    'Vi√™n s·ªßi': ['vi√™n s·ªßi', 'effervescent', 'c·ªëm s·ªßi b·ªçt'],
    'Vi√™n ng·∫≠m': ['vi√™n ng·∫≠m', 'sublingual'],
    'Vi√™n n√©n': ['vi√™n n√©n', 'vi√™n bao', 'tablet', 'n√©n bao', 'vi√™n nhai', 'vi√™n ph√¢n t√°n', 'vi√™n'],
    'Siro': ['siro', 'sir√¥', 'siiro', 'syrup'],
    'H·ªón d·ªãch u·ªëng': ['h·ªón d·ªãch u·ªëng', 'h·ªón d·ªãch', 'oral suspension', 'suspension'],
    'Nh≈© t∆∞∆°ng u·ªëng': ['nh≈© t∆∞∆°ng u·ªëng', 'nh≈© t∆∞∆°ng', 'nh≈© d·ªãch u·ªëng', 'oral emulsion', 'nh·ªè gi·ªçt'],
    'Dung d·ªãch u·ªëng': ['dung d·ªãch u·ªëng', 'oral solution', 'solution', 'thu·ªëc n∆∞·ªõc u·ªëng', 'thu·ªëc n∆∞·ªõc'],
    'Thu·ªëc c·ªëm u·ªëng': ['thu·ªëc c·ªëm', 'c·ªëm pha', 'granules', 'c·ªëm'],
    'Thu·ªëc b·ªôt u·ªëng': ['thu·ªëc b·ªôt pha u·ªëng', 'thu·ªëc b·ªôt', 'powder', 'b·ªôt pha u·ªëng', 'b·ªôt'],
    'Dung d·ªãch (Chung)': ['dung d·ªãch'],
    'D√πng ngo√†i (Chung)': ['d√πng ngo√†i', 'external', 'topical'],
    'Nguy√™n li·ªáu': ['nguy√™n li·ªáu', 'active ingredient'],
}

def classify_dangBaoChe_final(text):
    if pd.isnull(text): return "Kh√¥ng x√°c ƒë·ªãnh"
    s = unicodedata.normalize('NFKC', str(text).lower())
    s = re.sub(r'[^a-z0-9√†-·ªπ\s]', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    for standard_form, keywords in DEFINITIVE_DBC_MAP.items():
        if any(re.search(r'\b' + re.escape(keyword) + r'\b', s) for keyword in keywords):
            return standard_form
    return 'Kh√°c (Ch∆∞a ph√¢n lo·∫°i)'

def extract_quantity(text):
    numbers = re.findall(r'(\d+[\.,]?\d*)', str(text))
    if not numbers:
        return 1.0
    numbers = [float(n.replace(',', '.')) for n in numbers]
    quantity = np.prod(numbers)
    return quantity if quantity > 0 else 1.0

ULTIMATE_UNIT_CONVERSION_MAP = { 'g': 1_000, 'mg': 1, 'mcg': 0.001, 'ml': 1_000 }
UNIT_REGEX = r'(\d+[\.,]?\d*)\s*(mcg|¬µg|mg|g|kg|ml|l|iu|ui|%)'

def extract_ingredient_features_ultimate(row):
    hoatChat_str = str(row.get('hoatChat', '')).lower().strip().replace('/', ';')
    hamLuong_val = row.get('hamLuong')
    hoatChat_list = [hc.strip() for hc in hoatChat_str.split(';')] if hoatChat_str else []
    so_luong_hoat_chat = len(hoatChat_list)
    hoat_chat_chinh = hoatChat_list[0] if so_luong_hoat_chat > 0 else "kh√¥ng r√µ"
    if pd.isnull(hamLuong_val):
        return pd.Series([so_luong_hoat_chat, hoat_chat_chinh, 0.0, 0.0, 0.0], index=['so_luong_hoat_chat', 'hoat_chat_chinh', 'hl_chinh_mg', 'tong_hl_phu_mg', 'tong_hl_iu'])
    hamLuong_normalized = str(hamLuong_val).replace(',', '.')
    dosages = re.findall(UNIT_REGEX, hamLuong_normalized.lower())
    total_mg = 0.0
    total_iu = 0.0
    converted_dosages_mg = []
    for value_str, unit in dosages:
        value = float(value_str)
        if unit in ULTIMATE_UNIT_CONVERSION_MAP:
            converted_value = value * ULTIMATE_UNIT_CONVERSION_MAP[unit]
            converted_dosages_mg.append(converted_value)
        elif unit in ['iu', 'ui']:
            total_iu += value
    hl_chinh_mg = converted_dosages_mg[0] if len(converted_dosages_mg) > 0 else 0.0
    tong_hl_phu_mg = sum(converted_dosages_mg[1:]) if len(converted_dosages_mg) > 1 else 0.0
    return pd.Series([so_luong_hoat_chat, hoat_chat_chinh, hl_chinh_mg, tong_hl_phu_mg, total_iu], index=['so_luong_hoat_chat', 'hoat_chat_chinh', 'hl_chinh_mg', 'tong_hl_phu_mg', 'tong_hl_iu'])

def parse_user_query(query):
    parsed = {"tenThuoc": "N/A", "hoatChat": np.nan, "hamLuong": np.nan, "soLuong": "N/A", "donViTinh": "N/A"}
    temp_query = query
    match_hc = re.search(r'^(.*?)\s*\((.*?)\)', temp_query)
    if match_hc:
        parsed["hoatChat"] = match_hc.group(1).strip()  
        parsed["tenThuoc"] = match_hc.group(2).strip()
        temp_query = temp_query.replace(match_hc.group(0), parsed["tenThuoc"]) 
    else:
        parsed["tenThuoc"] = temp_query.strip() 

    hamluong_pattern = r'(\d+[\.,]?\d*\s*(?:mg|g|mcg|ml|l|iu|ui|kg)(?:\s*/\s*(?:ml|g|vi√™n))?)'
    match_hl = re.search(hamluong_pattern, temp_query, re.IGNORECASE)
    if match_hl:
        parsed["hamLuong"] = match_hl.group(1).strip()
        temp_query = temp_query.replace(match_hl.group(0), '')

    unit_keywords = ['vi√™n nang', 'vi√™n n√©n', 'nang', 'vi√™n', 'g√≥i', '·ªëng', 'chai', 'l·ªç', 'h·ªôp', 'tu√Ωp']
    unit_pattern_sl = '|'.join(unit_keywords)
    match_sl = re.search(r'(\d+)\s*(' + unit_pattern_sl + r')\b', temp_query, re.IGNORECASE)
    if match_sl:
        parsed["soLuong"] = f"{match_sl.group(1)} {match_sl.group(2)}"
        parsed["donViTinh"] = match_sl.group(2).capitalize()
        temp_query = temp_query.replace(match_sl.group(0), '')

    if parsed["tenThuoc"] == "N/A":
        parsed["tenThuoc"] = temp_query.strip()
    
    parsed["quyCachDongGoi"] = parsed["soLuong"] if pd.notna(parsed["soLuong"]) else parsed["tenThuoc"]
    return parsed
    
def parse_dosage_value(hamLuong_val):
    if pd.isnull(hamLuong_val):
        return 0.0
    hamLuong_normalized = str(hamLuong_val).replace(',', '.')
    dosages = re.findall(UNIT_REGEX, hamLuong_normalized.lower())
    total_mg = 0.0
    for value_str, unit in dosages:
        value = float(value_str)
        if unit in ULTIMATE_UNIT_CONVERSION_MAP:
            total_mg += value * ULTIMATE_UNIT_CONVERSION_MAP[unit]
    return total_mg

def get_packaging_type(text):
    text = str(text).lower()
    if 'l·ªç' in text: return 'l·ªç'
    if 'chai' in text: return 'chai'
    if 'tu√Ωp' in text or 'tube' in text: return 'tu√Ωp'
    if '·ªëng' in text: return '·ªëng'
    if 'v·ªâ' in text: return 'v·ªâ'
    if 'h·ªôp' in text: return 'h·ªôp'
    if 'g√≥i' in text: return 'g√≥i'
    if 't√∫i' in text: return 't√∫i'
    return 'kh√°c'

def get_base_unit(text):
    text = str(text).lower()
    if 'vi√™n nang' in text or 'nang' in text: return 'nang'
    if 'vi√™n' in text: return 'vi√™n'
    if 'ml' in text: return 'ml'
    if 'g' in text or 'gam' in text: return 'g'
    if 'g√≥i' in text: return 'g√≥i'
    if '·ªëng' in text: return '·ªëng'
    return 'kh√°c'


def transform_hybrid_data(hybrid_data, train_columns, target_maps, mean_price):
    df = pd.DataFrame([hybrid_data])

    df['doanhNghiepSanxuat_final'] = df['doanhNghiepSanxuat'].astype(str).str.lower().str.strip()
    df['nuocSanxuat_cleaned'] = df['nuocSanxuat'].astype(str).str.lower().str.strip()

    df['is_dangBaoChe_missing'] = df['dangBaoChe'].isnull().astype(int)
    df['dangBaoChe_final'] = df['dangBaoChe'].apply(classify_dangBaoChe_final)  
    df['soLuong'] = df['quyCachDongGoi'].apply(extract_quantity)
    df['loaiDongGoiChinh'] = df['quyCachDongGoi'].apply(get_packaging_type)
    df['donViCoSo'] = df['quyCachDongGoi'].apply(get_base_unit)

    ingredient_features = df.apply(extract_ingredient_features_ultimate, axis=1)
    df = pd.concat([df, ingredient_features], axis=1)

    column_name_mapping = {
        'doanhNghiepSanxuat': 'doanhNghiepSanxuat_final',
        'nuocSanxuat': 'nuocSanxuat_cleaned',
        'dangBaoChe': 'dangBaoChe_final',
        'hoat_chat_chinh': 'hoat_chat_chinh',
        'loaiDongGoiChinh': 'loaiDongGoiChinh',
        'donViCoSo': 'donViCoSo'
    }

    for original_col, final_col in column_name_mapping.items():
        if original_col in target_maps:
            target_map = target_maps[original_col]

            series_to_encode = df[final_col].astype(str).str.lower().str.strip()

            encoded_col_name = final_col + '_encoded'
            df[encoded_col_name] = series_to_encode.map(target_map).fillna(mean_price)
        else:
            pass
    df_final = df.reindex(columns=train_columns)
    df_final.fillna(0, inplace=True)
    
    return df_final

@st.cache_data

def call_gemini_parser(ocr_text):

    try:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
        model = genai.GenerativeModel('gemini-2.5-flash-lite')

        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        prompt = f"""
        Gi·ªù t√¥i g·ª≠i b·∫°n m·ªôt ƒëo·∫°n OCR t·ª´ toa thu·ªëc. Nhi·ªám v·ª• c·ªßa b·∫°n l√† li·ªát k√™ nh·ªØng thu·ªëc trong ƒë√≥ ra theo danh s√°ch t∆∞∆°ng ·ª©ng nh∆∞ sau:

        - T√™n thu·ªëc:
        - Ho·∫°t ch·∫•t:
        - H√†m l∆∞·ª£ng:
        - S·ªë l∆∞·ª£ng:
        - ƒê∆°n v·ªã t√≠nh:

        M·ªôt v√≠ d·ª• nh∆∞ sau:

        1. T√™n thu·ªëc: UNASYN
        Ho·∫°t ch·∫•t: Sultamicillin
        H√†m l∆∞·ª£ng: 375mg
        S·ªë l∆∞·ª£ng: 8 vi√™n
        ƒê∆°n v·ªã t√≠nh: Vi√™n

        2. T√™n thu·ªëc: NEXT G CAL
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh)
        H√†m l∆∞·ª£ng: (Ch∆∞a x√°c ƒë·ªãnh)
        S·ªë l∆∞·ª£ng: 30 vi√™n
        ƒê∆°n v·ªã t√≠nh: Vi√™n

        3. T√™n thu·ªëc: HEMO Q MOM,
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh),
        H√†m l∆∞·ª£ng: (Ch∆∞a x√°c ƒë·ªãnh),
        S·ªë l∆∞·ª£ng: 30 Vi√™n,
        ƒê∆°n v·ªã: Vi√™n

        4. T√™n thu·ªëc: POVIDINE,
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh),
        H√†m l∆∞·ª£ng: 10% 90ML,
        S·ªë l∆∞·ª£ng: 1 Chai,
        ƒê∆°n v·ªã: Chai

        5. T√™n thu·ªëc: B·ªô chƒÉm s√≥c r·ªën,
        Ho·∫°t ch·∫•t: (Ch∆∞a x√°c ƒë·ªãnh),
        H√†m l∆∞·ª£ng: (Ch∆∞a x√°c ƒë·ªãnh),
        S·ªë l∆∞·ª£ng: 1 B·ªô,
        ƒê∆°n v·ªã: B·ªô


        ·ªû nh·ªØng thu·ªëc c√≥ d·∫•u (), v√≠ d·ª• nh∆∞ Gliclazid (Staclazide 30 MR) th√¨ ph·∫ßn trong d·∫•u () l√† t√™n thu·ªëc, ph·∫ßn ·ªü ngo√†i l√† ho·∫°t ch·∫•t, nh∆∞ tr∆∞·ªùng h·ª£p n√†y th√¨ t√™n thu·ªëc l√† Staclazide 30 MR, ho·∫°t ch·∫•t l√† Gliclazid. kh√¥ng ƒë∆∞·ª£c ph√©p l·∫•y c·∫£ Gliclazid (Staclazide 30 MR) cho t√™n thu·ªëc. 

        C√≤n nh·ªØng tr∆∞·ªùng h·ª£p ch·ªâ c√≥ ch·ªØ in hoa nh∆∞ NEXT G CAL th√¨ NEXT G CAL l√† t√™n thu·ªëc lu√¥n. ch·ªØ O ngay tr∆∞·ªõc ƒë∆°n v·ªã nh∆∞ mg l√† s·ªë 0 b·ªã OCR nh·∫ßm.

        Tuy nhi√™n, n·∫øu thu·ªëc c√≥ () v√† c√≥ ch·ª©a c·∫£ ch·ªØ in hoa to√†n b·ªô, nh∆∞ YESOM 40 40mg (Esomeprazol 40mg) th√¨ ph·∫ßn trong ngo·∫∑c m·ªõi l√† ho·∫°t ch·∫•t, c√≤n ph·∫ßn ia hoa to√†n b·ªô lu√¥n lu√¥n l√† t√™n thu·ªëc. Nh∆∞ trong tr∆∞·ªùng h·ª£p n√†y th√¨ YESOM 40 l√† t√™n thu√¥c, Esomeprazol l√† ho·∫°t ch·∫•t.

        ƒê√¥i khi k·∫øt qu·∫£ OCR c≈©ng s·∫Ω b·ªã sai ch√≠nh t·∫£, v√≠ d·ª• 'Cac lon·ªâ vitamin' th√¨ b·∫°n s·ª≠a l·∫°i cho ƒë√∫ng l√† 'C√°c lo·∫°i vitamin'.
        
        B·∫°n hi·ªÉu ch∆∞a, v√† ch·ªâ tr·∫£ l·ªùi b·∫±ng c√°ch li·ªát k√™ ra t√™n thu·ªëc v√† c√°c gi√° tr·ªã t∆∞∆°ng ·ª©ng theo ƒë√∫ng ƒë·ªãnh d·∫°ng m√† v√≠ d·ª• t√¥i ƒë√£ cung c·∫•p. kh√¥ng tr·∫£ l·ªùi b·∫•t c·ª© g√¨ kh√°c. ch·ªâ tr·∫£ l·ªùi ph·∫ßn c·∫ßn thi·∫øt kh√¥ng gi·∫£i th√≠ch b·∫•t c·ª© g√¨ kh√°c, kh√¥ng m·ªü ngo·∫∑c ƒë·ªÅ ch√∫ th√≠ch. ph·∫£i tr·∫£ l·ªùi theo d·∫°ng li·ªát k√™ xu·ªëng d√≤ng nh∆∞ tr√™n. Nh·ªõ l√† ph·∫£i li·ªát k√™ ƒë·ªß s·ªë l∆∞·ª£ng ƒë·∫•y nh√©.
        OCR ƒë√¢y:
        ---
        {ocr_text}
        ---
        """
        
        response = model.generate_content(prompt, safety_settings=safety_settings)
        return response.text
    except Exception as e:
        st.error(f"L·ªói khi g·ªçi API c·ªßa Google AI: {e}")
        return None

def parse_gemini_response(response_text):

    parsed_drugs = []

    blocks = re.split(r'\n*(?=[-\d\.]*\s*T√™n thu·ªëc:)', response_text)
    
    for block in blocks:
        if not block.strip():
            continue

        ten_thuoc = re.search(r"T√™n thu·ªëc:\s*(.*)", block)
        hoat_chat = re.search(r"Ho·∫°t ch·∫•t:\s*(.*)", block)
        ham_luong = re.search(r"H√†m l∆∞·ª£ng:\s*(.*)", block)
        so_luong = re.search(r"S·ªë l∆∞·ª£ng:\s*(.*)", block)
        don_vi_tinh = re.search(r"ƒê∆°n v·ªã t√≠nh:\s*(.*)", block)

        drug_dict = {
            "tenThuoc": ten_thuoc.group(1).strip() if ten_thuoc and "(Ch∆∞a x√°c ƒë·ªãnh)" not in ten_thuoc.group(1) else np.nan,
            "hoatChat": hoat_chat.group(1).strip() if hoat_chat and "(Ch∆∞a x√°c ƒë·ªãnh)" not in hoat_chat.group(1) else np.nan,
            "hamLuong": ham_luong.group(1).strip() if ham_luong and "(Ch∆∞a x√°c ƒë·ªãnh)" not in ham_luong.group(1) else np.nan,
            "soLuong": so_luong.group(1).strip() if so_luong and "(Ch∆∞a x√°c ƒë·ªãnh)" not in so_luong.group(1) else np.nan,
            "donViTinh": don_vi_tinh.group(1).strip() if don_vi_tinh and "(Ch∆∞a x√°c ƒë·ªãnh)" not in don_vi_tinh.group(1) else np.nan,
        }

        if pd.notna(drug_dict["tenThuoc"]) or pd.notna(drug_dict["hoatChat"]):
            parsed_drugs.append(drug_dict)
        
    return parsed_drugs


st.title("G·ª£i √Ω Gi√° thu·ªëc")

model, scaler, target_maps, mean_price, df_full, train_cols = load_artifacts()

if df_full is not None:
    user_query_text = st.text_input("", placeholder="Nh·∫≠p t√™n thu·ªëc, v√≠ d·ª• Ho·∫°t ch·∫•t (t√™n thu·ªëc) h√†m l∆∞·ª£ng s·ªë l∆∞·ª£ng...", label_visibility="collapsed")
    uploaded_file = st.file_uploader("", type=["png", "jpg", "jpeg"], label_visibility="collapsed")

    query_to_process = None
    source = "text"

    if uploaded_file is not None:
        image_bytes = uploaded_file.getvalue()
        with st.spinner("ƒêang ƒë·ªçc ·∫£nh..."):
            ocr_text = " ".join(ocr_reader.readtext(image_bytes, detail=0))
        query_to_process = ocr_text
        source = "ocr"
        with st.expander("Xem to√†n b·ªô vƒÉn b·∫£n nh·∫≠n d·∫°ng ƒë∆∞·ª£c"):
            st.text_area("", ocr_text, height=150)
    elif user_query_text:
        query_to_process = user_query_text
        
    if query_to_process:
        st.markdown("---")
        with st.spinner("AI ƒëang ph√¢n t√≠ch ƒë∆°n thu·ªëc..."):
            if source == "ocr":
                structured_response = call_gemini_parser(query_to_process)
                if not structured_response:
                    st.error("AI kh√¥ng th·ªÉ ph√¢n t√≠ch vƒÉn b·∫£n t·ª´ ·∫£nh n√†y.")
                    st.stop()
                lines_to_process = parse_gemini_response(structured_response)
            else:
                lines_to_process = [parse_user_query(query_to_process)]
        
        if not lines_to_process:
            st.warning("Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c ƒë∆°n thu·ªëc n√†o.")
            st.stop()

        valid_drug_count = 0
        for i, parsed_info in enumerate(lines_to_process):
            search_key = f"{parsed_info.get('tenThuoc','')} {parsed_info.get('hoatChat','')}".strip()
            if not search_key or (pd.isna(parsed_info.get('tenThuoc')) and pd.isna(parsed_info.get('hoatChat'))):
                continue  
            
            valid_drug_count += 1
            if len(lines_to_process) > 1:
                st.markdown(f"--- \n ### üíä K·∫øt qu·∫£ cho ƒë∆°n thu·ªëc {valid_drug_count}")

            with st.spinner(f"ƒêang x·ª≠ l√Ω: '{search_key[:50]}...'"):
                st.markdown(f"**T√™n thu·ªëc:** {parsed_info.get('tenThuoc') or '(Ch∆∞a x√°c ƒë·ªãnh)'}")
                st.markdown(f"**Ho·∫°t ch·∫•t:** {parsed_info.get('hoatChat') or '(Ch∆∞a x√°c ƒë·ªãnh)'}")
                st.markdown(f"**H√†m l∆∞·ª£ng:** {parsed_info.get('hamLuong') or '(Ch∆∞a x√°c ƒë·ªãnh)'}")
                st.markdown(f"**S·ªë l∆∞·ª£ng:** {parsed_info.get('soLuong') or 'N/A'}")
                st.markdown(f"**ƒê∆°n v·ªã t√≠nh:** {parsed_info.get('donViTinh') or 'N/A'}")
                st.markdown("---")

                choices = df_full['tenThuoc'].dropna().tolist()
                best_match, score, _ = process.extractOne(search_key, choices)
                
                if not best_match:
                    st.warning("Kh√¥ng t√¨m th·∫•y thu·ªëc t∆∞∆°ng t·ª± trong CSDL.")
                else:
                    drug_info_row = df_full[df_full['tenThuoc'] == best_match].iloc[0]

                    if score >= 90:
                        can_extrapolate = False
                        if pd.notna(parsed_info.get('hoatChat')) and pd.notna(parsed_info.get('hamLuong')):
                            user_hc_clean = str(parsed_info.get('hoatChat', '')).lower()
                            db_hc_clean = str(drug_info_row.get('hoatChat', '')).lower()
                            if fuzz.partial_ratio(user_hc_clean, db_hc_clean) > 80:
                                user_dosage_mg = parse_dosage_value(parsed_info.get('hamLuong'))
                                db_dosage_mg = parse_dosage_value(drug_info_row.get('hamLuong'))
                                if user_dosage_mg > 0 and db_dosage_mg > 0 and user_dosage_mg != db_dosage_mg:
                                    can_extrapolate = True
                        
                        if can_extrapolate:
                            ratio = user_dosage_mg / db_dosage_mg
                            st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `Ngo·∫°i suy theo h√†m l∆∞·ª£ng (T·ª∑ l·ªá: {ratio:.2f}x)`")
                            st.caption(f"D·ª±a tr√™n gi√° c·ªßa *{best_match}* (ƒë·ªô t∆∞∆°ng ƒë·ªìng t√™n: {score:.0f}%)")
                            gia_kk_base = drug_info_row['giaBanBuonDuKien']
                            gia_tt_base = drug_info_row.get('giaThanh', np.nan)
                            gia_kk_extrapolated = gia_kk_base * ratio
                            gia_tt_extrapolated = gia_tt_base * ratio if pd.notna(gia_tt_base) else np.nan
                            st.metric("Gi√° K√™ Khai (∆Ø·ªõc t√≠nh)", f"{gia_kk_extrapolated:,.0f} VND" if pd.notna(gia_kk_base) else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                            st.metric("Gi√° Th·ªã Tr∆∞·ªùng (∆Ø·ªõc t√≠nh)", f"{gia_tt_extrapolated:,.0f} VND" if pd.notna(gia_tt_base) else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                        else:
                            st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `Levenshtein distance (Thu·ªëc t∆∞∆°ng ƒë·ªìng: {best_match}, ƒë·ªô t∆∞∆°ng ƒë·ªìng: {score:.0f}%)`")
                            gia_kk = drug_info_row['giaBanBuonDuKien']
                            gia_tt = drug_info_row.get('giaThanh', np.nan)
                            st.metric("Gi√° K√™ Khai", f"{gia_kk:,.0f} VND" if pd.notna(gia_kk) else "Kh√¥ng c√≥ d·ªØ li·ªáu")
                            st.metric("Gi√° Th·ªã Tr∆∞·ªùng", f"{gia_tt:,.0f} VND" if pd.notna(gia_tt) else "Kh√¥ng c√≥ d·ªØ li·ªáu")

                    else:
                        st.markdown(f"**Ph∆∞∆°ng th·ª©c:** `XGBoost Regressor`")
                        st.caption(f"S·ª≠ d·ª•ng th√¥ng tin b·ªï sung (nh√† SX, n∆∞·ªõc SX, D·∫°ng b√†o ch·∫ø...) t·ª´ thu·ªëc t∆∞∆°ng t·ª± nh·∫•t: *{best_match}*")
                        
                        hybrid_data = {
                            'hoatChat': parsed_info.get('hoatChat') or drug_info_row.get('hoatChat'),
                            'hamLuong': parsed_info.get('hamLuong') or drug_info_row.get('hamLuong'),
                            'quyCachDongGoi': parsed_info.get('quyCachDongGoi') or drug_info_row.get('quyCachDongGoi'),
                            'doanhNghiepSanxuat': drug_info_row.get('doanhNghiepSanxuat'),
                            'nuocSanxuat': drug_info_row.get('nuocSanxuat'),
                            'dangBaoChe': drug_info_row.get('dangBaoChe')
                        }
                        
                        try:
                            transformed_data = transform_hybrid_data(hybrid_data, train_cols, target_maps, mean_price)
                            scaled_data = scaler.transform(transformed_data)
                            prediction_log = model.predict(scaled_data)
                            prediction = np.expm1(prediction_log)
                            gia_kk_pred, gia_tt_pred = prediction[0][0], prediction[0][1]
                            st.metric("Gi√° K√™ Khai (D·ª± ƒëo√°n)", f"{gia_kk_pred:,.0f} VND")
                            st.metric("Gi√° Th·ªã Tr∆∞·ªùng (D·ª± ƒëo√°n)", f"{gia_tt_pred:,.0f} VND")
                        except Exception as e:
                            st.error(f"L·ªói khi d·ª± ƒëo√°n: {e}")
